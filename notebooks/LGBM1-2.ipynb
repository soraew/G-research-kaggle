{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basics\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os \n",
    "import random\n",
    "# import gresearch_crypto\n",
    "\n",
    "# plotting\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ml shit\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import make_scorer\n",
    "# stat shit\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics import tsaplots\n",
    "# models\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "\n",
    "data_root = \"../data/\"\n",
    "SEED = 2021\n",
    "REMOVE_LB_TEST_OVERLAPPING_DATA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting all seeds\n",
    "def fix_all_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "fix_all_seeds(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(data_root+\"train.csv\")\n",
    "asset_d = pd.read_csv(data_root+\"asset_details.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REMOVE_LB_TEST_OVERLAPPING_DATA:\n",
    "    train[\"datetime\"] = pd.to_datetime(train[\"timestamp\"], unit=\"s\")\n",
    "    train = train[train[\"datetime\"]<\"2021-06-13 00:00:00\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic functions\n",
    "\n",
    "#this one is prob redundant\n",
    "to_datetime = lambda timestamp: datetime.strftime(datetime.fromtimestamp(timestamp),\"%Y-%m-%d\")\n",
    "\n",
    "get_month_timestamp = lambda timestamp: to_datetime(timestamp).month\n",
    "\n",
    "get_month = lambda datetime: datetime.month\n",
    "\n",
    "get_month_day = lambda datetime: datetime.strftime(\"%m-%d\")\n",
    "\n",
    "############# Two new features from the competition tutorial\n",
    "def log_return(series, periods=1):\n",
    "    return np.log(series).diff(periods=periods)\n",
    "\n",
    "def roll(array, shift):\n",
    "    # this supposebly improves the performance of np.roll\n",
    "    if not (isinstance(array, np.ndarray)):\n",
    "        array = np.asarray(array)\n",
    "    idx = shift%len(array)\n",
    "    return np.concatenate([array[-idx:], array[:-idx]])\n",
    "\n",
    "\n",
    "############# from lightGBT tutorial\n",
    "def upper_shadow(df):\n",
    "    return df['High'] - np.maximum(df['Close'], df['Open'])\n",
    "\n",
    "def lower_shadow(df):\n",
    "    return np.minimum(df['Close'], df['Open']) - df['Low']\n",
    "\n",
    "############# realized here needs to use some data for calculation of initial values\n",
    "def realized(close, N=240):\n",
    "    rt = list(np.log(C_t / C_t_1) for C_t, C_t_1 in zip(close[1:], close[:-1]))\n",
    "    rt_mean = sum(rt) / len(rt)\n",
    "    return np.sqrt(sum((r_i - rt_mean) ** 2 for r_i in rt) * N / (len(rt) - 1))\n",
    "\n",
    "########## function for calling all feature creating functions\n",
    "def get_features(df, Lag=True):\n",
    "    df_feat = df[[\"Count\", \"Open\",\"High\", \"Low\", \"Close\", \"Volume\",\"VWAP\", \"Target\"]].copy()\n",
    "    \n",
    "    df_feat[\"Upper_shadow\"] = upper_shadow(df_feat)\n",
    "    df_feat[\"Lower_shadow\"] = lower_shadow(df_feat)\n",
    "\n",
    "    df_feat[\"Volume\"] = log_return(df_feat[\"Volume\"]) # maybe Volume is fine just like that(?)\n",
    "    df_feat[\"Count\"] = log_return(df_feat[\"Count\"])\n",
    "    df_feat[\"VWAP\"] = log_return(df_feat[\"VWAP\"])\n",
    "    df_feat = df_feat[1:] # compensate\n",
    "    if Lag:\n",
    "        for lag in range(1, 6):\n",
    "            roll_feature = \"VWAP\"\n",
    "            df_feat[\"rolled_\"+roll_feature+f\"_{lag}\"] = \\\n",
    "                roll(df_feat[roll_feature].values, lag)\n",
    "    \n",
    "    ########### for now, simple dropna()     ########### \n",
    "    ########### later we can use reindexing  ########### \n",
    "    df_feat.dropna(inplace=True)\n",
    "    # df_feat = df_feat.reindex(range(btc.index[0],btc.index[-1]+60, 60), method=\"pad\")\n",
    "\n",
    "    return df_feat\n",
    "\n",
    "\n",
    "def Xy_model_asset(train, asset_id, Lag=True):\n",
    "    df = train[train[\"Asset_ID\"]==asset_id]\n",
    "\n",
    "    # todo : try different features here \n",
    "    #        also, scale the features\n",
    "    df_proc = get_features(df, Lag)\n",
    "\n",
    "    df_proc[\"y\"] = df[\"Target\"]\n",
    "    # 念の為\n",
    "    df_proc.dropna(how=\"any\", inplace=True)\n",
    "    X = df_proc.drop(\"y\", axis=1)\n",
    "    y = df_proc[\"y\"]\n",
    "\n",
    "    # todo : try different models here\n",
    "    # model = LGBMRegressor()\n",
    "    # model.fit(X, y)\n",
    "    return X, y#, model\n",
    "\n",
    "def weighted_correlation(a, b, weights): # a is preds, b is gt\n",
    "\n",
    "  w = np.ravel(weights)\n",
    "  a = np.ravel(a)\n",
    "  b = np.ravel(b)\n",
    "\n",
    "  sum_w = np.sum(w)\n",
    "  mean_a = np.sum(a * w) / sum_w\n",
    "  mean_b = np.sum(b * w) / sum_w\n",
    "  var_a = np.sum(w * np.square(a - mean_a)) / sum_w\n",
    "  var_b = np.sum(w * np.square(b - mean_b)) / sum_w\n",
    "\n",
    "  cov = np.sum((a * b * w)) / np.sum(w) - mean_a * mean_b\n",
    "  corr = cov / np.sqrt(var_a * var_b)\n",
    "\n",
    "  return corr\n",
    "\n",
    "def scorer(model, X, y):\n",
    "    # not sure how this line works, just trying\n",
    "    y_pred = model.predict([X])[0]\n",
    "    score = np.corrcoef(y_pred, y)\n",
    "    return score\n",
    "\n",
    "\n",
    "my_scorer = make_scorer(scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_assets = train.groupby(\"Asset_ID\")\n",
    "# asset_dict = {name:train_assets[group] for name, group in zip(asset_d.Asset_Name, asset_d.Asset_ID)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We first create simple LGBM Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'> <class 'pandas.core.series.Series'>\n",
      "[0.95929106 0.96415332 0.88945699 0.95953581 0.91125602] \n",
      " 0.9367386392096602\n"
     ]
    }
   ],
   "source": [
    "model = LGBMRegressor()\n",
    "X, y = Xy_model_asset(train, 1)\n",
    "print(type(X), type(y))\n",
    "scores = cross_val_score(model, X, y, cv = 5)#, scoring=scorer)\n",
    "print(scores, \"\\n\", np.mean(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Count</th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Volume</th>\n      <th>VWAP</th>\n      <th>Target</th>\n      <th>Upper_shadow</th>\n      <th>Lower_shadow</th>\n      <th>rolled_VWAP_1</th>\n      <th>rolled_VWAP_2</th>\n      <th>rolled_VWAP_3</th>\n      <th>rolled_VWAP_4</th>\n      <th>rolled_VWAP_5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10</th>\n      <td>0.025864</td>\n      <td>13835.036000</td>\n      <td>14052.30000</td>\n      <td>13680.00</td>\n      <td>13828.102000</td>\n      <td>-0.016092</td>\n      <td>0.000961</td>\n      <td>-0.015037</td>\n      <td>217.264000</td>\n      <td>148.102000</td>\n      <td>0.000032</td>\n      <td>0.001131</td>\n      <td>-0.001567</td>\n      <td>-0.000113</td>\n      <td>-0.000100</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.809511</td>\n      <td>13823.900000</td>\n      <td>14000.40000</td>\n      <td>13601.00</td>\n      <td>13801.314000</td>\n      <td>0.572973</td>\n      <td>-0.002481</td>\n      <td>-0.010309</td>\n      <td>176.500000</td>\n      <td>200.314000</td>\n      <td>0.000961</td>\n      <td>0.000032</td>\n      <td>0.001131</td>\n      <td>-0.001567</td>\n      <td>-0.000113</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>-0.193750</td>\n      <td>13802.512000</td>\n      <td>13999.00000</td>\n      <td>13576.28</td>\n      <td>13768.040000</td>\n      <td>-0.350538</td>\n      <td>-0.001629</td>\n      <td>-0.008999</td>\n      <td>196.488000</td>\n      <td>191.760000</td>\n      <td>-0.002481</td>\n      <td>0.000961</td>\n      <td>0.000032</td>\n      <td>0.001131</td>\n      <td>-0.001567</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>0.534003</td>\n      <td>13766.000000</td>\n      <td>13955.90000</td>\n      <td>13554.44</td>\n      <td>13724.914000</td>\n      <td>1.028847</td>\n      <td>-0.003489</td>\n      <td>-0.008079</td>\n      <td>189.900000</td>\n      <td>170.474000</td>\n      <td>-0.001629</td>\n      <td>-0.002481</td>\n      <td>0.000961</td>\n      <td>0.000032</td>\n      <td>0.001131</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>-0.292185</td>\n      <td>13717.714000</td>\n      <td>14000.70000</td>\n      <td>13520.00</td>\n      <td>13717.112000</td>\n      <td>-0.426825</td>\n      <td>-0.002087</td>\n      <td>-0.004422</td>\n      <td>282.986000</td>\n      <td>197.112000</td>\n      <td>-0.003489</td>\n      <td>-0.001629</td>\n      <td>-0.002481</td>\n      <td>0.000961</td>\n      <td>0.000032</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>22221627</th>\n      <td>-0.105361</td>\n      <td>35512.214286</td>\n      <td>35557.00000</td>\n      <td>35495.20</td>\n      <td>35518.808571</td>\n      <td>0.006912</td>\n      <td>-0.000100</td>\n      <td>0.002982</td>\n      <td>38.191429</td>\n      <td>17.014286</td>\n      <td>-0.000669</td>\n      <td>0.000286</td>\n      <td>0.001095</td>\n      <td>0.000421</td>\n      <td>-0.002192</td>\n    </tr>\n    <tr>\n      <th>22221641</th>\n      <td>0.265868</td>\n      <td>35518.764114</td>\n      <td>35560.00000</td>\n      <td>35444.30</td>\n      <td>35479.547218</td>\n      <td>0.542715</td>\n      <td>-0.000113</td>\n      <td>0.002965</td>\n      <td>41.235886</td>\n      <td>35.247218</td>\n      <td>-0.000100</td>\n      <td>-0.000669</td>\n      <td>0.000286</td>\n      <td>0.001095</td>\n      <td>0.000421</td>\n    </tr>\n    <tr>\n      <th>22221655</th>\n      <td>0.358498</td>\n      <td>35476.735920</td>\n      <td>35500.10369</td>\n      <td>35420.32</td>\n      <td>35451.233956</td>\n      <td>-0.051945</td>\n      <td>-0.001567</td>\n      <td>0.002798</td>\n      <td>23.367770</td>\n      <td>30.913956</td>\n      <td>-0.000113</td>\n      <td>-0.000100</td>\n      <td>-0.000669</td>\n      <td>0.000286</td>\n      <td>0.001095</td>\n    </tr>\n    <tr>\n      <th>22221669</th>\n      <td>-0.461749</td>\n      <td>35456.720000</td>\n      <td>35540.70000</td>\n      <td>35443.09</td>\n      <td>35523.640000</td>\n      <td>-0.406577</td>\n      <td>0.001131</td>\n      <td>0.002177</td>\n      <td>17.060000</td>\n      <td>13.630000</td>\n      <td>-0.001567</td>\n      <td>-0.000113</td>\n      <td>-0.000100</td>\n      <td>-0.000669</td>\n      <td>0.000286</td>\n    </tr>\n    <tr>\n      <th>22221683</th>\n      <td>0.275381</td>\n      <td>35520.585714</td>\n      <td>35573.40000</td>\n      <td>35443.20</td>\n      <td>35551.520000</td>\n      <td>0.532539</td>\n      <td>0.000032</td>\n      <td>0.002776</td>\n      <td>21.880000</td>\n      <td>77.385714</td>\n      <td>0.001131</td>\n      <td>-0.001567</td>\n      <td>-0.000113</td>\n      <td>-0.000100</td>\n      <td>-0.000669</td>\n    </tr>\n  </tbody>\n</table>\n<p>1811995 rows × 15 columns</p>\n</div>",
      "text/plain": "             Count          Open         High       Low         Close  \\\n10        0.025864  13835.036000  14052.30000  13680.00  13828.102000   \n18        0.809511  13823.900000  14000.40000  13601.00  13801.314000   \n26       -0.193750  13802.512000  13999.00000  13576.28  13768.040000   \n34        0.534003  13766.000000  13955.90000  13554.44  13724.914000   \n42       -0.292185  13717.714000  14000.70000  13520.00  13717.112000   \n...            ...           ...          ...       ...           ...   \n22221627 -0.105361  35512.214286  35557.00000  35495.20  35518.808571   \n22221641  0.265868  35518.764114  35560.00000  35444.30  35479.547218   \n22221655  0.358498  35476.735920  35500.10369  35420.32  35451.233956   \n22221669 -0.461749  35456.720000  35540.70000  35443.09  35523.640000   \n22221683  0.275381  35520.585714  35573.40000  35443.20  35551.520000   \n\n            Volume      VWAP    Target  Upper_shadow  Lower_shadow  \\\n10       -0.016092  0.000961 -0.015037    217.264000    148.102000   \n18        0.572973 -0.002481 -0.010309    176.500000    200.314000   \n26       -0.350538 -0.001629 -0.008999    196.488000    191.760000   \n34        1.028847 -0.003489 -0.008079    189.900000    170.474000   \n42       -0.426825 -0.002087 -0.004422    282.986000    197.112000   \n...            ...       ...       ...           ...           ...   \n22221627  0.006912 -0.000100  0.002982     38.191429     17.014286   \n22221641  0.542715 -0.000113  0.002965     41.235886     35.247218   \n22221655 -0.051945 -0.001567  0.002798     23.367770     30.913956   \n22221669 -0.406577  0.001131  0.002177     17.060000     13.630000   \n22221683  0.532539  0.000032  0.002776     21.880000     77.385714   \n\n          rolled_VWAP_1  rolled_VWAP_2  rolled_VWAP_3  rolled_VWAP_4  \\\n10             0.000032       0.001131      -0.001567      -0.000113   \n18             0.000961       0.000032       0.001131      -0.001567   \n26            -0.002481       0.000961       0.000032       0.001131   \n34            -0.001629      -0.002481       0.000961       0.000032   \n42            -0.003489      -0.001629      -0.002481       0.000961   \n...                 ...            ...            ...            ...   \n22221627      -0.000669       0.000286       0.001095       0.000421   \n22221641      -0.000100      -0.000669       0.000286       0.001095   \n22221655      -0.000113      -0.000100      -0.000669       0.000286   \n22221669      -0.001567      -0.000113      -0.000100      -0.000669   \n22221683       0.001131      -0.001567      -0.000113      -0.000100   \n\n          rolled_VWAP_5  \n10            -0.000100  \n18            -0.000113  \n26            -0.001567  \n34             0.001131  \n42             0.000032  \n...                 ...  \n22221627      -0.002192  \n22221641       0.000421  \n22221655       0.001095  \n22221669       0.000286  \n22221683      -0.000669  \n\n[1811995 rows x 15 columns]"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[1.        , 0.97950478],\n",
      "       [0.97950478, 1.        ]]), array([[1.        , 0.98203391],\n",
      "       [0.98203391, 1.        ]]), array([[1.        , 0.94337342],\n",
      "       [0.94337342, 1.        ]]), array([[1.        , 0.97956864],\n",
      "       [0.97956864, 1.        ]]), array([[1.        , 0.95460958],\n",
      "       [0.95460958, 1.        ]])]\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "model = LGBMRegressor()\n",
    "scores = []\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    scores.append(np.corrcoef(y_pred, y_test))\n",
    "\n",
    "print(scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4  5  6  7  8  9 10 11 12 13 14 15 16 17 18] [0 1 2 3]\n",
      "[ 0  1  2  3  8  9 10 11 12 13 14 15 16 17 18] [4 5 6 7]\n",
      "[ 0  1  2  3  4  5  6  7 12 13 14 15 16 17 18] [ 8  9 10 11]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 16 17 18] [12 13 14 15]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15] [16 17 18]\n"
     ]
    }
   ],
   "source": [
    "list = np.zeros(19)\n",
    "kf.split(list)\n",
    "for i, j in kf.split(list):\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('pyflux': conda)",
   "name": "python3812jvsc74a57bd0b8bf3bbb95d33652ea8a09e83516ae388afde8f9530fb9552010d87506ab4938"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "metadata": {
   "interpreter": {
    "hash": "b8bf3bbb95d33652ea8a09e83516ae388afde8f9530fb9552010d87506ab4938"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}