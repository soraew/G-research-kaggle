{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "# import gresearch_crypto\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd, numpy as np\n",
    "from tensorflow.keras import layers\n",
    "# import tensorflow_probability as tfp # -> Error : annot import name 'naming' from 'tensorflow.python.autograph.core' \n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# relative imports\n",
    "# from get_feats import get_features\n",
    "from helperfuncs import get_time_fractions\n",
    "\n",
    "#basics \n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(DATA_ROOT, nrows):\n",
    "    df = pd.read_csv(DATA_ROOT+\"train.csv\", nrows=nrows)\n",
    "\n",
    "    assets = pd.read_csv(DATA_ROOT+\"asset_details.csv\")\n",
    "    df_grouped = df.groupby(\"Asset_ID\")\n",
    "\n",
    "    # we will work with other 12 assets\n",
    "    btc = df_grouped.get_group(1)\n",
    "    eth = df_grouped.get_group(6)\n",
    "\n",
    "    btc.set_index(\"timestamp\", inplace=True)\n",
    "    btc = btc.drop(columns=[\"Target\", \"Asset_ID\"])\n",
    "    btc = btc.add_suffix(\"_btc\")\n",
    "    eth.set_index(\"timestamp\", inplace=True)\n",
    "    eth = eth.drop(columns=[\"Target\", \"Asset_ID\"])\n",
    "    eth = eth.add_suffix(\"_eth\")\n",
    "\n",
    "    # working(1/3:15)\n",
    "    # btc_eth = pd.concat([btc, eth], join=\"outer\", axis=1)\n",
    "    # btc_eth_index = btc_eth.index.unique()\n",
    "\n",
    "\n",
    "    \n",
    "    #filling nans for now(reindex and drop later)\n",
    "    # getting rid of filling nans\n",
    "    # btc_eth.fillna(method=\"ffill\", inplace=True)\n",
    "\n",
    "    df_features = btc_eth.copy()\n",
    "    suffixes = [\"_btc\", \"_eth\"]\n",
    "    for suffix in suffixes:\n",
    "        df_features[\"logprice\"+suffix] = np.log(df_features[\"Close\"+suffix]) \n",
    "        df_features[\"Volatility\"+suffix] = np.log(df_features[\"High\"+suffix])\\\n",
    "            - np.log(df_features[\"Close\"+suffix])\n",
    "        df_features = df_features.drop(columns=[\"Close\"+suffix, \"High\"+suffix,\\\n",
    "            \"Low\"+suffix, \"Open\"+suffix, \"VWAP\"+suffix])\n",
    "\n",
    "    datetimes = pd.Series(df_features.index).astype(\"datetime64[s]\")\n",
    "    df_features[\"frac_day\"], df_features[\"frac_week\"], df_features['frac_of_month'], \\\n",
    "        df_features['frac_of_year'] = zip(*datetimes.map(get_time_fractions))\n",
    "\n",
    "\n",
    "    # calculate 2-asset targets(not 14 asset target)\n",
    "    df_logprices = df_features[[\"logprice_btc\", \"logprice_eth\"]]\n",
    "    # 𝑅𝑎(𝑡)=𝑙𝑜𝑔(𝑃𝑎(𝑡+16) / 𝑃𝑎(𝑡+1))=𝑙𝑜𝑔(𝑃𝑎(𝑡+16)−𝑙𝑜𝑔(𝑃𝑎(𝑡+1)\n",
    "    df_returns = df_logprices.shift(-16) - df_logprices.shift(-1)\n",
    "    for suffix in suffixes:\n",
    "        df_returns.rename(columns={\"logprice\"+suffix : \"R\"+suffix}, inplace=True)\n",
    "\n",
    "    # find a better way to write next line\n",
    "    assets =  assets[(assets[\"Asset_ID\"] == 1) | (assets[\"Asset_ID\"] == 6)]\n",
    "    assets = assets.sort_values(by=[\"Asset_ID\"])\n",
    "    weights = assets[\"Weight\"].to_numpy()\n",
    "    weights = weights.reshape(len(weights), 1)\n",
    "\n",
    "\n",
    "    R = df_returns.to_numpy()# to array\n",
    "    weights_sum = np.sum(weights)\n",
    "    M = np.dot(R, weights) / weights_sum # weighted average => log_btc*w_btc + log_eth*w_eth\n",
    "    df_M = pd.DataFrame(data=M, index=df_returns.index, columns=[\"M\"])\n",
    "    R.shape,weights.shape, M.shape\n",
    "\n",
    "\n",
    "    df_R_M = df_returns.copy()\n",
    "    for col in df_R_M.columns:\n",
    "        df_R_M[col] = df_R_M[col] * df_M[\"M\"] # calculated R・M here\n",
    "    for suffix in suffixes:\n",
    "        df_R_M.rename(columns={\"R\"+suffix:\"R_M\"+suffix}, inplace=True)\n",
    "    df_R_M_rolling = df_R_M.rolling(window=3750).mean()\n",
    "\n",
    "\n",
    "    # creating M^2 \n",
    "    df_M2 = df_M ** 2\n",
    "    df_M2.rename(columns={\"M\" : \"M2\"}, inplace = True)\n",
    "    df_M2_rolling = df_M2.rolling(window=3750).mean()\n",
    "    df_betas = df_R_M_rolling.copy()    \n",
    "    for col in df_betas.columns: # columns = [R_M_btc\tR_M_eth]   \n",
    "        df_betas[col] = df_betas[col] / df_M2_rolling[\"M2\"] # caculating <R・M>/<M^2> here\n",
    "    for suffix in suffixes: # beta = <R・M>/<M^2> \n",
    "        df_betas.rename(columns={\"R_M\"+suffix : \"beta\"+suffix}, inplace = True)\n",
    "    df_targets = df_returns.copy()\n",
    "    for suffix in suffixes:\n",
    "        df_targets[\"R\"+suffix] -= df_betas[\"beta\"+suffix] * df_M[\"M\"] # R^a - β^a\n",
    "        df_targets.rename(columns={\"R\"+suffix: \"Target\"+suffix}, inplace=True)\n",
    "\n",
    "    df_features_targets = pd.concat([df_features, df_betas, df_targets], axis=1)\n",
    "    df_features_targets = df_features_targets.iloc[3750:-16] # drop nan rows\n",
    "\n",
    "    df_features_targets[\"datetime\"] = df_features_targets.index\n",
    "    df_features_targets[\"datetime\"] = df_features_targets[\"datetime\"].apply(datetime.fromtimestamp)\n",
    "\n",
    "    return df_features_targets\n",
    "\n",
    "# Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype.name\n",
    "        \n",
    "        if col_type not in ['object', 'category', 'datetime64[ns, UTC]', 'datetime64[ns]']:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df\n",
    "\n",
    "def reindex(df, index):\n",
    "    df = df.reindex(range(index[0], index[-1]+60), method=\"nearest\")\n",
    "    df = df.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"../data/\"\n",
    "\n",
    "\n",
    "\n",
    "DEVICE = \"CPU\"\n",
    "SEED = 42\n",
    "EPOCHS = 10\n",
    "DEBUG = True\n",
    "N_ASSETS = 14\n",
    "WINDOW_SIZE = 15\n",
    "BATCH_SIZE = 1024\n",
    "PCT_VALIDATION = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 9.82 MB\n",
      "Memory usage after optimization is: 3.27 MB\n",
      "Decreased by 66.7%\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "\n",
    "week_secs = 60*60*24*7\n",
    "month_secs = 60*60*24*31\n",
    "day_secs = 60*60*24\n",
    "# dataset is formed by 14 assets\n",
    "if DEBUG:\n",
    "    df = get_features(DATA_ROOT, nrows=week_secs)\n",
    "else:\n",
    "    df = get_features(DATA_ROOT, nrows=None)\n",
    "\n",
    "df = reduce_mem_usage(df)\n",
    "\n",
    "# getting rid of \"beta_eth\", \"Target_eth\" \n",
    "df = df[['Count_btc', 'Volume_btc', 'Count_eth', 'Volume_eth', 'logprice_btc',\n",
    "       'Volatility_btc', 'logprice_eth', 'Volatility_eth', 'frac_day',\n",
    "       'frac_week', 'frac_of_month', 'frac_of_year', 'beta_btc',\n",
    "       'Target_btc']]\n",
    "\n",
    "# adding datetime column\n",
    "if DEBUG:\n",
    "    df_vis = df.copy()\n",
    "    df_vis[\"datetime\"] = df_vis.index\n",
    "    df_vis.datetime = df_vis.datetime.apply(datetime.fromtimestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# about training\n",
    "LOAD_STRICT = True\n",
    "\n",
    "INC2021 = 0\n",
    "INC2020 = 0\n",
    "INC2019 = 0\n",
    "INC2018 = 0\n",
    "INC2017 = 0\n",
    "INCCOMP = 1\n",
    "INCSUPP = 0\n",
    "\n",
    "train = df\n",
    "sample_prediction = pd.read_csv(DATA_ROOT+\"example_sample_submission.csv\")\n",
    "\n",
    "# the notebooks uses upper/lower_shadow, open_sub_close, seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1952639, 14)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "reindex() missing 1 required positional argument: 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c5/nv2shz796rnc77pvdpyqmhwm0000gn/T/ipykernel_10656/1886047519.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/pyflux/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   8738\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8739\u001b[0m         )\n\u001b[0;32m-> 8740\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8742\u001b[0m     def applymap(\n",
      "\u001b[0;32m~/miniforge3/envs/pyflux/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/pyflux/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/pyflux/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    826\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m                 \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m                     \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: reindex() missing 1 required positional argument: 'index'"
     ]
    }
   ],
   "source": [
    "train = train.sort_index()\n",
    "index = train.index.unique()\n",
    "print(train.shape)\n",
    "train = train.apply(reindex).reset_index(0, drop=True).sort_index()\n",
    "gc.collect()\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit",
   "name": "python3812jvsc74a57bd0b8bf3bbb95d33652ea8a09e83516ae388afde8f9530fb9552010d87506ab4938"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "metadata": {
   "interpreter": {
    "hash": "b8bf3bbb95d33652ea8a09e83516ae388afde8f9530fb9552010d87506ab4938"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}